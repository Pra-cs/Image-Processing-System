# ğŸ–¼ï¸ Scalable Image Processing System

This project is a scalable system that crawls webpages, extracts images, downloads them, generates metadata (e.g., resolution, size, format), and stores both images and metadata. It includes REST APIs to initiate crawling and view results, with an extensible frontend.

---

## ğŸ”§ Tech Stack

- **Backend**: Python, FastAPI
- **Frontend**: React.js
- **Database**: PostgreSQL
- **ORM**: SQLAlchemy
- **Image Handling**: Pillow
- **HTTP Requests**: httpx
- **Web Crawling**: BeautifulSoup
- **Containerization**: Docker, Docker Compose

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py             # FastAPI app
â”‚   â”‚   â”œâ”€â”€ models.py           # SQLAlchemy models
â”‚   â”‚   â”œâ”€â”€ schemas.py          # Pydantic schemas
â”‚   â”‚   â”œâ”€â”€ database.py         # DB connection
â”‚   â”‚   â”œâ”€â”€ crawler.py          # Crawling logic
â”‚   â”‚   â”œâ”€â”€ image_utils.py      # Image downloading & metadata
â”‚   â”‚   â””â”€â”€ routes.py           # API endpoints
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js
â”‚   â”‚   â”œâ”€â”€ index.js
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ SubmitForm.js
â”‚   â”‚       â””â”€â”€ ResultGrid.js
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/your-username/image-processor.git
cd image-processor
```

---

### 2. Run with Docker (Recommended)

Make sure Docker and Docker Compose are installed.

```bash
docker-compose up --build
```

- Backend: [http://localhost:8000/docs](http://localhost:8000/docs)
- Frontend: [http://localhost:3000](http://localhost:3000)

---

### 3. Without Docker (Manual)

#### Backend

```bash
cd backend
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --reload
```

#### Frontend

```bash
cd frontend
npm install
npm start
```

---

## ğŸ§  Features

- ğŸŒ **Webpage Crawling**: Parses a webpage and discovers all image URLs.
- ğŸ–¼ï¸ **Image Downloading**: Downloads and stores images locally.
- ğŸ§¾ **Metadata Extraction**: Calculates and stores metadata (dimensions, size, format).
- ğŸ”„ **Idempotency**: Avoids re-processing already crawled URLs.
- ğŸ“Š **REST APIs**: Initiate crawling, check results, and fetch image metadata.
- ğŸ§© **Frontend UI**: Submit URL, view downloaded images and metadata.
- ğŸ§© **Extensibility**: Easy to plug in new processing logic (e.g., ML image classification).

---

## ğŸ”Œ API Endpoints

- `POST /api/crawl`  
  Start crawling a new webpage.

  ```json
  { "url": "https://example.com" }
  ```

- `GET /api/results?url=https://example.com`  
  Fetch all images and metadata for a URL.

- `GET /api/images/:id`  
  Serve the image by ID.

---

## ğŸ–¥ï¸ Frontend Usage

1. Open [http://localhost:3000](http://localhost:3000)
2. Enter a webpage URL in the input box.
3. Click "Submit"
4. View the downloaded images and metadata below.

---

## ğŸ“ Example Metadata

```json
{
  "url": "https://example.com/images/logo.png",
  "width": 200,
  "height": 100,
  "format": "PNG",
  "size_kb": 45
}
```

---

## ğŸ”’ Security Notes

- URL validation is done to prevent SSRF.
- Image size and content-type are validated before saving.

---

## ğŸ“ˆ Future Improvements

- [ ] Image deduplication (hash comparison)
- [ ] Pagination for large result sets
- [ ] Search/filter on metadata (resolution, format)
- [ ] Add async queue (e.g., Celery/RQ)
- [ ] Cloud storage support (S3)

---

## ğŸ‘¨â€ğŸ’» Contributing

Pull requests are welcome. Please follow good commit hygiene and test your changes.

---

## ğŸ›¡ï¸ License

MIT License Â© 2025 Prabhat Singh
